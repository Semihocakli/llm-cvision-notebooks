{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzWVPKXsf0MR"
      },
      "outputs": [],
      "source": [
        "!pip install pandas python-dotenv langchain langchain-huggingface langchain-community faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcW0JX_Ff9Ug"
      },
      "outputs": [],
      "source": [
        "!apt-get install libfaiss-dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygysA3DYScB7"
      },
      "outputs": [],
      "source": [
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSkpzWoSUstu",
        "outputId": "02b9969d-505d-40d6-a84b-a7fbb1addab1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.26.4)\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8incSSKGTG13"
      },
      "outputs": [],
      "source": [
        "# Gerekli kütüphanelerin yüklenmesi\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "# Çevresel değişkenleri yükle\n",
        "load_dotenv()\n",
        "\n",
        "# API anahtarını ayarlayın\n",
        "os.environ['HUGGINGFACE_API_KEY'] = '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'  # Buraya kendi API anahtarınızı gireceksiniz.\n",
        "\n",
        "# 1. Adım: JSON dosyasını yükleyin ve veriyi işleyin\n",
        "# JSON dosyasının yolunu belirtin\n",
        "json_file_path = '/content/keskop_answer.json'  # JSON dosyanızın doğru yolunu yazın\n",
        "\n",
        "# JSON dosyasını okuyun\n",
        "with open(json_file_path, 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Soruları ve cevapları işlemek için boş listeler oluşturun\n",
        "questions = []\n",
        "answers = []\n",
        "\n",
        "# Her bir ana soruyu ve varyasyonlarını işleme\n",
        "for key, qa_list in data.items():\n",
        "    for qa in qa_list:\n",
        "        questions.append(qa['question'])\n",
        "        answers.append(qa['answer'])\n",
        "\n",
        "# Veriyi bir pandas DataFrame'e dönüştürmek (isteğe bağlı)\n",
        "df = pd.DataFrame({'question': questions, 'answer': answers})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xkp-04aT2uO"
      },
      "outputs": [],
      "source": [
        "# 2. Adım: Veriyi LangChain Document formatına dönüştürme\n",
        "documents = [Document(page_content=question, metadata={'answer': answer}) for question, answer in zip(questions, answers)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXeyRZujT03a"
      },
      "outputs": [],
      "source": [
        "# 3. Adım: Embeddings (Metin Gömme) ve FAISS Vektör Deposu Oluşturma\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n",
        "# FAISS vektör deposunu oluşturuyoruz\n",
        "vector_store = FAISS.from_documents(documents, embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IuuUwm7TzH4",
        "outputId": "29d4ecf8-56e3-440b-fe1e-a7dcd0dff3f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "# 4. Adım: LLM Ayarları (Hugging Face Hub üzerinden Mistral-7B modeli)\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"mistralai/Mistral-Nemo-Instruct-2407\",\n",
        "    huggingfacehub_api_token=os.environ['HUGGINGFACE_API_KEY'],\n",
        "    temperature=0.1,\n",
        "    model_kwargs={\"max_length\": 64}\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEHU_YkYTxzB"
      },
      "outputs": [],
      "source": [
        "# 5. Adım: BM25Retriever ile Alıcı (Retriever) Oluşturma\n",
        "retriever = BM25Retriever.from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVYGKbE0TwpX"
      },
      "outputs": [],
      "source": [
        "# 6. Adım: Özel bir prompt template oluşturma\n",
        "custom_prompt = PromptTemplate.from_template(\n",
        "    \"Aşağıdaki talimatları takip ederek soruyu yanıtla:\\n\\n\"\n",
        "    \"{instructions}\\n\\n\"\n",
        "    \"Soru: {question}\\n\"\n",
        "    \"Bağlam: {context}\\n\\n\"\n",
        "    \"Yanıt:\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7uMfjx3TuVc"
      },
      "outputs": [],
      "source": [
        "# 7. Adım: Retrieval-Augmented Generation (RAG) QA Zinciri Oluşturma\n",
        "rag_chain = (\n",
        "    {\n",
        "        \"context\": retriever,  # Kullanıcının sorusuna en uygun cevabı bulmak için retriever\n",
        "        \"question\": RunnablePassthrough(),  # Kullanıcının sorusunu geçirin\n",
        "        \"instructions\": lambda _: \"Aşağıdaki talimatları takip ederek soruyu yanıtlayın:\"  # Talimatlar (sabit veya özelleştirilebilir)\n",
        "    }\n",
        "    | custom_prompt  # Prompt şablonuyla birleştirme\n",
        "    | llm  # LLM ile yanıt üretme\n",
        "    | StrOutputParser()  # Yanıtı işleyip döndürme\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mt1Nl3crTs6D",
        "outputId": "e494ad1e-7506-479a-9553-963245e073b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Adınıza kayıtlı gayrimenkul yoksa, başka birine ait gayrimenkul ipoteği verebilirsiniz.\n"
          ]
        }
      ],
      "source": [
        "# Soru sormak için fonksiyon\n",
        "def ask_question(query):\n",
        "    try:\n",
        "        result = rag_chain.invoke(query)  # RAG zincirini çalıştırarak kullanıcıdan gelen soruya yanıt üretme\n",
        "        return result  # Üretilen yanıtı döndür\n",
        "    except Exception as e:\n",
        "        return f\"Bir hata oluştu: {str(e)}\"  # Hata durumunda hatayı döndür\n",
        "\n",
        "# Ana fonksiyon: Soru sorma\n",
        "if __name__ == \"__main__\":\n",
        "    query = \"İpotek verebilecek bir gayrimenkulum olmadığı için ne yapmam gerekiyor?\"  # Örnek bir soru\n",
        "    answer = ask_question(query)  # Soruyu RAG zinciri ile yanıtla\n",
        "    print(answer)  # Yanıtı yazdır\n"
      ]
    }
  ]
}