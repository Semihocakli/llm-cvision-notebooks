{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yhz3FEnvIjtf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from dotenv import load_dotenv\n",
        "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "os.environ['HUGGINGFACE_API_KEY'] = '...'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_file_path = '/content/keskop_answer.json'\n",
        "\n",
        "with open(json_file_path, 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "questions = []\n",
        "answers = []\n",
        "\n",
        "for key, qa_list in data.items():\n",
        "    for qa in qa_list:\n",
        "        questions.append(qa['question'])\n",
        "        answers.append(qa['answer'])\n",
        "\n",
        "df = pd.DataFrame({'question': questions, 'answer': answers})"
      ],
      "metadata": {
        "id": "OSATZD6wJe1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Improved preprocess function\n",
        "def preprocess_text(text):\n",
        "    # Tokenize and remove stopwords\n",
        "    stop_words = set(stopwords.words('turkish'))\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    return ' '.join([token for token in tokens if token not in stop_words])\n",
        "\n",
        "# Preprocess questions\n",
        "df['preprocessed_question'] = df['question'].apply(preprocess_text)\n",
        "\n",
        "documents = [Document(page_content=question, metadata={'answer': answer, 'preprocessed': prep})\n",
        "             for question, answer, prep in zip(df['question'], df['answer'], df['preprocessed_question'])]\n",
        "\n",
        "\"\"\"\n",
        "* Text Split – Chunking ( Metin Bölme - Parçalama ) veya veriye göre en uygun işlem uygunalabilir.\n",
        "- Parçalama, büyük veri bağlamındaki karmaşıklığı ve çeşitliliği azaltmak için büyük veri topluluğunu daha küçük bölümlere ayırma stratejisidir.\n",
        "  Parçalama, token boyutuna göre daha küçük veri parçalarını böler.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "jnbj7U9uJlA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a more powerful multilingual model\n",
        "model_name = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': True}\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n",
        "\"\"\"\n",
        "* metin gömme modeli daha güçlü ve türkçe kelimeler üstünde daha iyi çalışabilen bir model olabilir.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "53FiIoknJonw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store = FAISS.from_documents(documents, embeddings)\n",
        "\"\"\"\n",
        "* veriye ve modele en uygun veritabanı seçimi yapılabilir.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "lOXunuQ8JrHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "    huggingfacehub_api_token=os.environ['HUGGINGFACE_API_KEY'],\n",
        "    temperature=0.1,\n",
        "    model_kwargs={\"max_length\": 256}\n",
        ")\n",
        "\"\"\"\n",
        "* Model seçimi kullanılabilen en parametreli model olması iyi.\n",
        "* temparature -> 0.1 - 0.3 arası iyi olabilir.\n",
        "* top_p, veya top_k parametreleri eklenmesi, yanıtların çeşitliliği ve kalitesi üzerinde etkili olabilir.\n",
        "* max_length değerini artırmak, modelin daha fazla bellek (RAM) ve hesaplama kaynağı kullanmasına neden olur.\n",
        "  - Büyük uzunluklar, özellikle sınırlı donanım kaynaklarında, bellek hatalarına veya çok yavaş yanıt sürelerine yol açabilir.\n",
        "  - Daha uzun girişler, yanıt kalitesini artırabilir, ancak aynı zamanda modelin dikkat mekanizmasının zorlanmasına ve dikkat dağılmasına da neden olabilir.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "shebRmagJ6px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
        "\"\"\"\n",
        "* \"k\" parametresi, arama sırasında döndürülecek sonuç sayısını belirtir.\n",
        "  - retriever'ın daha fazla sonuç döndürmesini sağlar, böylece modelin daha fazla bilgiye erişerek daha iyi bir yanıt oluşturması mümkün olur.\n",
        "  - Ancak, çok fazla sonuç döndürmek de performansı etkileyebilir ve gereksiz bilgilerin değerlendirilmesine yol açabilir.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Ei57UWnTJ-8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_prompt = PromptTemplate.from_template(\n",
        "    \"Aşağıdaki talimatları takip ederek soruyu yanıtla:\\n\\n\"\n",
        "    \"{instructions}\\n\\n\"\n",
        "    \"Soru: {question}\\n\"\n",
        "    \"Bağlam: {context}\\n\\n\"\n",
        "    \"Yanıt:\"\n",
        ")\n",
        "\"\"\"\n",
        "* prompt iyileştirmesi yapılabilir.\n",
        " - Kullanıcı talimatlarını daha açıklayıcı bir şekilde güncelleyerek modelin doğru şekilde yanıt vermesi sağlanabilir.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "TPldyl9bKE9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain = (\n",
        "    {\n",
        "        \"context\": retriever,\n",
        "        \"question\": RunnablePassthrough(),\n",
        "        \"instructions\": lambda _: \"Verilen bağlamı kullanarak soruyu yanıtlayın. Eğer soru ve bağlam arasında bir ilişki yoksa, 'Üzgünüm, bu soruyu yanıtlayamıyorum.' deyin.\"\n",
        "    }\n",
        "    | custom_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\"\"\"\n",
        "* instructions belirlenebilir.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "vHB6366yKHP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_similarity(query, document):\n",
        "    query_embedding = embeddings.embed_query(query)\n",
        "    doc_embedding = embeddings.embed_query(document.page_content)\n",
        "    return cosine_similarity([query_embedding], [doc_embedding])[0][0]\n",
        "\n",
        "def find_most_similar_question(query, documents, threshold=0.7):\n",
        "    preprocessed_query = preprocess_text(query)\n",
        "    similarities = [calculate_similarity(preprocessed_query, doc) for doc in documents]\n",
        "    max_similarity = max(similarities)\n",
        "    if max_similarity >= threshold:\n",
        "        most_similar_index = similarities.index(max_similarity)\n",
        "        return documents[most_similar_index], max_similarity\n",
        "    return None, max_similarity\n",
        "\n",
        "\"\"\"\n",
        "* nlp ile soruları ve çıktıları iyileştirebilirsek iyi olur.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "GRsxAsdeKMkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_question(query):\n",
        "    try:\n",
        "        most_similar_doc, similarity = find_most_similar_question(query, documents)\n",
        "\n",
        "        if most_similar_doc:\n",
        "            result = rag_chain.invoke(query)\n",
        "            return f\"Soru: {query}\\nEn benzer soru: {most_similar_doc.page_content}\\nBenzerlik skoru: {similarity:.2f}\\nCevap: {result}\"\n",
        "        else:\n",
        "            return f\"Üzgünüm. Bu soruya benzer bir soru bulamadım (Benzerlik skoru: {similarity:.2f}). Lütfen başka bir şekilde sormayı deneyin veya farklı bir soru sorun.\"\n",
        "    except Exception as e:\n",
        "        return f\"Bir hata oluştu: {str(e)}\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Soru-cevap sistemine hoş geldiniz! Çıkmak için 'q' yazın.\")\n",
        "    while True:\n",
        "        query = input(\"\\nLütfen bir soru sorun: \")\n",
        "        if query.lower() == 'q':\n",
        "            print(\"Sistemden çıkılıyor. İyi günler!\")\n",
        "            break\n",
        "        answer = ask_question(query)\n",
        "        print(answer)"
      ],
      "metadata": {
        "id": "ZPyT4GhUKPnJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}