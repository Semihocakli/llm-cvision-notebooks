{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Bu tarz bir görev için çok popüler ve değerli olan gömme modelleri (embedding models) and vektör deposu (vector store) kullanılmaya başlanılacaktır.\n",
        "Bir ürün kataloğunu ilgilenilen öğeler için sorgulamanıza izin veren bir araç buna örnek olarak verilebilir."
      ],
      "metadata": {
        "id": "8glaF2DKC66f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install --upgrade langchain\n",
        "import os\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # lokal .env dosyasını oku\n",
        "\n",
        "from langchain.chains import RetrievalQA # soru cevap zinciri\n",
        "from langchain.chat_models import ChatOpenAI # open ai chat modeli\n",
        "from langchain.document_loaders import CSVLoader # csv dosyası yüklemek için\n",
        "from langchain.vectorstores import DocArrayInMemorySearch # vektör deposu\n",
        "from IPython.display import display, Markdown # bilgiyi göstermek için"
      ],
      "metadata": {
        "id": "ZUNpWMu7C-wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = 'OutdoorClothingCatalog_1000.csv' # https://s172-31-4-92p30982.lab-aws-production.deeplearning.ai/edit/OutdoorClothingCatalog_1000.csv#\n",
        "loader = CSVLoader(file_path=file)"
      ],
      "metadata": {
        "id": "PDLWrDt0DBf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.indexes import VectorstoreIndexCreator # Bu indeks vektör deposu oluşturmayı kolaylaştırır"
      ],
      "metadata": {
        "id": "ziJNwJLfDdzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install docarray\n",
        "index = VectorstoreIndexCreator(\n",
        "    vectorstore_cls=DocArrayInMemorySearch\n",
        ").from_loaders([loader]) # from_loaders doküman yükleyiciler listesi döner(bu örnekte 1 tane gönderiyoruz)"
      ],
      "metadata": {
        "id": "fGbwAJNIDfde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query =\"Lütfen güneş korumalı tüm gömleklerinizi indirimli bir tabloda \\\n",
        "listeleyin ve her birini özetleyin.\""
      ],
      "metadata": {
        "id": "m6Ud4vOkDg5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = index.query(query)\n",
        "display(Markdown(response)) # Güneş korumalı gömlek isimleri ve açıklamaları ile birlikte en sonda bir özet tablo gösterir"
      ],
      "metadata": {
        "id": "IuIcNK9lDjR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = CSVLoader(file_path=file)\n",
        "docs = loader.load()\n",
        "docs[0]"
      ],
      "metadata": {
        "id": "LnwL0OyKDktk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM'ler bir anda yalnızca birkaç bin kelimeyi inceleyebilir (parça(chunk)halinde), çok fazla dokümanımız varken soru cevap yapacak bir LLM'i nasıl sağlarız? İşte burada devreye gömme modelleri ve vektör depoları girer.\n",
        "Gömme denilen işlemde kelimeler onları temsil edecek ve semantik (anlamsal) değerlerini tutacak vektörler (-0.003530 gibi) ile değiştirilirler. Doküman chunk denen parçalara bölünür ve o chunk'lardaki metinler vektöre embedding yöntemi ile çevrilir ve bunlar vektör veritabanına kaydedilir. Böylece bir prompt sorgusu geldiğinde o sorgu embed'e çevirlir ve tüm embedded chunk'lar içinde veritabanında aratılır ve n en benzer chunk içeriden alınıp dönülür. Bu dönülen değerler artık LLM bağlamına sığabilir ve böylece modelden çıktımızı alabiliriz."
      ],
      "metadata": {
        "id": "qXSqP2gyDojq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "embed = embeddings.embed_query(\"Merhaba benim adım Harrison\")\n",
        "\n",
        "print(len(embed)) # 1536 embed vardır\n",
        "\n",
        "print(embed[:5])  # Her bir embed vektör tutar"
      ],
      "metadata": {
        "id": "z2Sf99t_DuLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = DocArrayInMemorySearch.from_documents(\n",
        "    docs, # doküman listesi\n",
        "    embeddings # Embedding nesnesi\n",
        ")"
      ],
      "metadata": {
        "id": "DZgz1grQD0Ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Lütfen güneşi önleyen bir gömlek önerin\""
      ],
      "metadata": {
        "id": "mu1wF4PiD1z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = db.similarity_search(query) # Benzerliğe göre vektör deposunda arama yapar böylece dokümanları alırız\n",
        "\n",
        "len(docs) # Doküman sayısı\n",
        "\n",
        "docs[0]   # İlk doküman"
      ],
      "metadata": {
        "id": "2mOwFkGRD3HO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alıcı (retriever), belgeleri döndüren bir sorguyu alan herhangi bir yöntemle desteklenebilen genel bir arabirimdir. Vektör depoları ve gömme modellerinden başka daha karmaşık veya basit yöntemler de vardır."
      ],
      "metadata": {
        "id": "O-BQVI3nD-wP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = db.as_retriever()\n",
        "\n",
        "llm = ChatOpenAI(temperature = 0.0)\n",
        "\n",
        "# Dokümanları tek bir metin parçasına birleştiriyoruz\n",
        "qdocs = \"\".join([docs[i].page_content for i in range(len(docs))])\n",
        "\n",
        "response = llm.call_as_llm(f\"{qdocs} Soru: Lütfen güneş korumalı tüm \\\n",
        "gömleklerinizi bir tablo halinde listeleyin ve her birini özetleyin.\")\n",
        "\n",
        "display(Markdown(response))"
      ],
      "metadata": {
        "id": "MOBj2ECuEAAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stuffing (Doldurma)\n",
        "Doldurma, dil modeline göndermek için tüm verileri bağlam biçiminde promptlara doldurduğumuz basit bir yöntemdir. Doldurma, LLM'e tek bir çağrı yapmayı mümkün kılar. LLM'in tüm verilere aynı anda erişimi vardır ancak LLM'lerin bir bağlam uzunluğu vardır. Büyük veya çok sayıda belge için bu, bağlamdan daha büyük bir promptla sonuçlanacağı için çalışmaz. Anlaması kolay, yeterince iyi çalışan, maaliyetsiz bir yöntemdir ancak her zaman iyi çalışmaz örneğin aynı tarz soru yanıtlama işlemini birçok chunk üzerinde yapmak istiyorsak ne yapacağız?\n",
        "Doldurma yöntemi en popüleri olsa da kullanabileceğimiz 3 çeşit başka yöntem daha bulunmaktadır:\n",
        "\n",
        "\n",
        "1.   **Map_reduce (harita indirgeme)**\n",
        "Tüm chunk'ları alır ve soruyla birlikte LLM'e iletir. Ardından yanıtı alır ve tüm bireysel yanıtları nihai bir yanıtta özetlemek için başka bir dil model çağrısı kullanır. Bu gerçekten güçlü bir yöntemdir çünkü herhangi bir sayıda belge üzerinde kullanabilirsiniz. Chunkların her biri ayrı ayrı LLM'e soru yöneltir, bundan dolayı da çok daha fazla çağrı gerektirir ve her belgeyi ayrı ayrı ele almak çok arzu edilen bir yöntem değildir. Özetleme için tercih edilen bir yöntemdir.\n",
        "\n",
        "2.   **Refine (arıtma)**\n",
        "Birçok dokümana bakmak için kullanılan başka bir yöntemdir, bir önceki dokümandan alınan cevap üzerine kurularak çalışır. Zamanla cevapları farklı dokümanlardan toplayıp kombine etmek için uygun bir yöntemdir. Genellikle daha uzun cevapların daha yavaş sürede alınmasına neden olan bir yöntemdir. Yavaştır çünkü artık çağrılar bağımsızdır ve bir önceki çağrının sonucuna bağımlı oldukları için onları beklerler.\n",
        "\n",
        "3. **Map_rerank (harita yeniden sıralaması)**\n",
        "Tüm çağrıların bağımsız olduğu bu yöntem çok daha deneysel ve ilginçtr. Her doküman için LLM'e tek bir çağrı atılır ve skor dönmesi istenir. Ve en yüksek skorlu olan seçilir.\n"
      ],
      "metadata": {
        "id": "7TkSPo3iEBZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_stuff = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "query =  \"Lütfen güneş korumalı tüm gömleklerinizi indirimli \\\n",
        "bir tabloda listeleyin ve her birini özetleyin.\"\n",
        "\n",
        "response = qa_stuff.run(query)\n",
        "\n",
        "display(Markdown(response))\n",
        "\n",
        "response = index.query(query, llm=llm)\n",
        "\n",
        "index = VectorstoreIndexCreator(\n",
        "    vectorstore_cls=DocArrayInMemorySearch,\n",
        "    embedding=embeddings,\n",
        ").from_loaders([loader])"
      ],
      "metadata": {
        "id": "PhX9AIwjEZDG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}